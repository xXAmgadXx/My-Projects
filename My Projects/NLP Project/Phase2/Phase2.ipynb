{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypJOP65MToeo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ffc5c39-9797-4c23-d48d-1367a5be04c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=b28f20438907dec9a5bb9b0c7436dbe355599f3549c9fc708aed1ba0ebf682dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "id": "CvGl2JsBWFev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efdcf9c8-d41c-4c8e-ab18-600309ac973e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n"
      ],
      "metadata": {
        "id": "uFowHSV-ZWpQ",
        "outputId": "9f28e10b-1510-42b2-d165-b55bfb4aebc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "documents=[\n",
        "    \"Artifitial Inteligence\",\n",
        "    \"Neural Networks\"\n",
        "    \"Deep Learning\",\n",
        "]\n",
        "data=\"\"\n",
        "\n",
        "for title in documents:\n",
        "  document_content = wikipedia.summary(title , sentences=200)\n",
        "  data +=document_content"
      ],
      "metadata": {
        "id": "OFUzXJj-USNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJxz5ngSU4g_",
        "outputId": "eadebfb7-1ddc-41e2-f272-a84a19a00d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and uses learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\n",
            "AI technology is widely used throughout industry, government, and science. Some high-profile applications include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\n",
            "Alan Turing was the first person to conduct substantial research in the field that he called machine intelligence. Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture. This led to the AI boom of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.Deep learning is the subset of machine learning methods based on neural networks with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.\n",
            "Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
            "Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, in particular the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low quality models for that purpose.\n",
            "\n",
            "\n",
            "== Overview ==\n",
            "Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\n",
            "Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a slightly more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data))"
      ],
      "metadata": {
        "id": "sc1kZ75TS9jL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d28123-b7d5-405c-f82b-afa56dbce01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ZWbKuwLhWoWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c300caa3-d275-4205-934c-e99b87cf3840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of English stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# Tokenize the text and remove stopwords and punctuation\n",
        "tokens = word_tokenize(data)\n",
        "data_words = [word.lower() for word in tokens if word.lower() not in stop_words and word.lower() not in punctuation]\n"
      ],
      "metadata": {
        "id": "p1ZuQejZWtBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMMdpD4kXNie",
        "outputId": "caf33fd3-ae2f-4cc3-ff6d-43108503ee2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "embedding_dim = 100\n",
        "\n",
        "# Create a dictionary to store embeddings, where keys are words and values are embeddings\n",
        "embeddings_dict = {}\n",
        "\n",
        "for word in data_words:\n",
        "    if word in word_vectors:\n",
        "        embeddings_dict[word] = word_vectors[word]\n",
        "    else:\n",
        "        # Handle the case when the word is not in the vocabulary by generating a random embedding\n",
        "        embeddings_dict[word] = np.random.rand(embedding_dim)\n",
        "\n",
        "# Example of embeddings_dict: {'word1': embedding_vector1, 'word2': embedding_vector2, ...}\n"
      ],
      "metadata": {
        "id": "zHZvU0V0YCEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8bb6a0-eca3-4a72-971d-65b88b3344ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of words in the dictionary:\", len(embeddings_dict))# Convert embeddings dictionary values to numpy array\n",
        "X = np.array(list(embeddings_dict.values()))\n",
        "\n",
        "# Example of X shape\n",
        "print(\"Shape of X:\", X.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN6t9CUdU3pi",
        "outputId": "37e6b951-2bd5-40f0-a509-469359e7d940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the dictionary: 255\n",
            "Shape of X: (255, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
        "\n",
        "# Assuming X_train contains a single sequence of word embeddings\n",
        "X_train = X_train.reshape(1, X_train.shape[0], X_train.shape[1])\n",
        "X_test = X_test.reshape(1, X_test.shape[0], X_test.shape[1])\n"
      ],
      "metadata": {
        "id": "F38Vdwmr1qk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "if X_train.size == 0 or np.all(X_train == 0):\n",
        "    print(\"Training data is empty. Cannot train the model.\")\n",
        "else:\n",
        "    # Create labels by shifting X_train by one step\n",
        "    y_train = np.roll(X_train, -1, axis=1)\n",
        "    # The last element in y_train will be the same as the first element in X_train, so we remove it\n",
        "    y_train[:, -1, :] = 0\n",
        "\n",
        "    # Assuming X_train contains a single sequence of word embeddings\n",
        "    number_of_words = X_train.shape[1]  # Number of words in the sequence\n",
        "    embedding_dim = X_train.shape[2]  # Embedding dimension\n",
        "\n",
        "    # Define RNN model\n",
        "    model = Sequential([\n",
        "        SimpleRNN(units=64, return_sequences=True, input_shape=(None, embedding_dim)),\n",
        "        Dense(embedding_dim)  # Output layer with the same dimension as input (to predict the next word embedding)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mae')  # Use MAE loss for predicting embeddings\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=4)\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "GIFc2Ig1IXUO",
        "outputId": "159a44a3-d7b0-480f-d2bd-7089a28a1a40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.5821\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.5700\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.5597\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.5504\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.5418\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.5336\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.5256\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.5180\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.5107\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.5035\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, None, 64)          10560     \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 100)         6500      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17060 (66.64 KB)\n",
            "Trainable params: 17060 (66.64 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "1vtJjA31Z8hQ",
        "outputId": "1341d425-6049-4b5c-97c9-b7f62e404a92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to save model summary as PDF\n",
        "def save_model_summary_to_pdf(model, architecture_name):\n",
        "    # Save model summary to a string\n",
        "    summary_str = []\n",
        "    model.summary(print_fn=lambda x: summary_str.append(x))\n",
        "    summary_str = \"\\n\".join(summary_str)\n",
        "\n",
        "    # Create a plot for model summary\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.text(0.1, 0.5, summary_str, {'fontsize': 10}, fontfamily='monospace')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Save the plot as PDF\n",
        "    pdf_file_path = f'/content/drive/MyDrive/{architecture_name}_model_summary.pdf'\n",
        "    plt.savefig(pdf_file_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "350ufDQOZZQW",
        "outputId": "4287ce90-d262-4a5f-cd19-0709ae431191",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model summary to PDF for the first architecture\n",
        "save_model_summary_to_pdf(model, 'word_level_RNN')\n"
      ],
      "metadata": {
        "id": "ac_SUUgubYdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzRMXj5n-p36",
        "outputId": "f20b49a6-8bc1-48fa-b061-73e3e0f190e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 51, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape X_test and X_test_labels to match the expected input shape\n",
        "\n",
        "# Prepare labels for the test set\n",
        "X_test_labels = np.roll(X_test, -1, axis=1)\n",
        "X_test_labels[:, -1, :] = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "9xdZ8GYi6BYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKrv1U8i_T1P",
        "outputId": "2c7139f7-5e4b-40ce-9521-628bfeb6f518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 204, 100)\n",
            "(1, 204, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = model.evaluate(X_test, X_test_labels)  # Assuming reconstruction loss\n",
        "print(\"Test loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNsmrENx6Bak",
        "outputId": "05605509-f005-4dd9-e3a8-fd7e71c138be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 369ms/step - loss: 0.5242\n",
            "Test loss: 0.5241793990135193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def predict_next_word(word, model, embeddings_dict):\n",
        "    # Get the embedding of the input word from the embeddings dictionary\n",
        "    word_embedding = embeddings_dict[word]\n",
        "\n",
        "    # Reshape the word embedding to match the input shape of the model\n",
        "    word_embedding = np.expand_dims(word_embedding, axis=0)  # Add batch dimension\n",
        "    word_embedding = np.expand_dims(word_embedding, axis=0)  # Add sequence dimension\n",
        "\n",
        "    # Predict the embedding of the next word\n",
        "    next_word_embedding = model.predict(word_embedding)\n",
        "\n",
        "    # Remove the batch and sequence dimensions\n",
        "    next_word_embedding = np.squeeze(next_word_embedding, axis=0)\n",
        "\n",
        "    # Convert embeddings_dict values to a 2D numpy array\n",
        "    embeddings_array = np.stack(list(embeddings_dict.values()))\n",
        "\n",
        "    # Calculate cosine similarity between the predicted embedding and embeddings in the embeddings dictionary\n",
        "    similarities = cosine_similarity(next_word_embedding, embeddings_array)\n",
        "\n",
        "    # Find the index of the word with the highest similarity\n",
        "    closest_word_index = np.argmax(similarities)\n",
        "\n",
        "    # Get the word corresponding to the index\n",
        "    closest_word = list(embeddings_dict.keys())[closest_word_index]\n",
        "\n",
        "    return closest_word\n",
        "\n",
        "# Example usage\n",
        "seed_word = \"intelligence\"\n",
        "predicted_word = predict_next_word(seed_word, model, embeddings_dict)\n",
        "print(\"Predicted next word:\", predicted_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIQfRGTRE3NU",
        "outputId": "7f0676ac-598e-4fc6-cb21-a0a5d8106d27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 284ms/step\n",
            "Predicted next word: modern\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "seed_word = \"intelligence\"\n",
        "predicted_word = predict_next_word(seed_word, model, embeddings_dict)\n",
        "print(\"Predicted next word:\", predicted_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLrHQ6_DFDmB",
        "outputId": "543a808f-c936-47c2-c061-8f27d35d9df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "Predicted next word: modern\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##* perform prediction based character*#"
      ],
      "metadata": {
        "id": "FRiHiw-ihBRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_characters(text):\n",
        "    return [char for char in text if char.strip() and ord(char) not in [44, 39]]  # 44 is the Unicode code point for ',' and 39 is for \"'\"\n",
        "\n",
        "data_characters = tokenize_characters(data.lower())\n",
        "\n",
        "char_to_index = {char: i for i, char in enumerate(sorted(set(data_characters)))}\n",
        "\n",
        "# Create the reverse mapping from index to characters\n",
        "index_to_char = {i: char for char, i in char_to_index.items()}\n",
        "\n",
        "# Number of unique characters\n",
        "num_unique_chars = len(char_to_index)\n",
        "\n",
        "# Define character embedding dimension\n",
        "embedding_dim = 100\n",
        "\n",
        "# Generate random embeddings for characters\n",
        "embeddings_dict = {}\n",
        "for char, index in char_to_index.items():\n",
        "    embeddings_dict[char] = np.random.rand(embedding_dim)\n"
      ],
      "metadata": {
        "id": "ArtfJ5hIGFbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR3bkpKzpHRp",
        "outputId": "2200e25b-a5b6-41a4-863f-8af0aa7706e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', '(', 'a', 'i', ')', 'i', 'n', 'i', 't', 's', 'b', 'r', 'o', 'a', 'd', 'e', 's', 't', 's', 'e', 'n', 's', 'e', 'i', 's', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', 'e', 'x', 'h', 'i', 'b', 'i', 't', 'e', 'd', 'b', 'y', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 's', 'p', 'a', 'r', 't', 'i', 'c', 'u', 'l', 'a', 'r', 'l', 'y', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', 's', 'y', 's', 't', 'e', 'm', 's', '.', 'i', 't', 'i', 's', 'a', 'f', 'i', 'e', 'l', 'd', 'o', 'f', 'r', 'e', 's', 'e', 'a', 'r', 'c', 'h', 'i', 'n', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', 's', 'c', 'i', 'e', 'n', 'c', 'e', 't', 'h', 'a', 't', 'd', 'e', 'v', 'e', 'l', 'o', 'p', 's', 'a', 'n', 'd', 's', 't', 'u', 'd', 'i', 'e', 's', 'm', 'e', 't', 'h', 'o', 'd', 's', 'a', 'n', 'd', 's', 'o', 'f', 't', 'w', 'a', 'r', 'e', 't', 'h', 'a', 't', 'e', 'n', 'a', 'b', 'l', 'e', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 's', 't', 'o', 'p', 'e', 'r', 'c', 'e', 'i', 'v', 'e', 't', 'h', 'e', 'i', 'r', 'e', 'n', 'v', 'i', 'r', 'o', 'n', 'm', 'e', 'n', 't', 'a', 'n', 'd', 'u', 's', 'e', 's', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', 'a', 'n', 'd', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', 't', 'o', 't', 'a', 'k', 'e', 'a', 'c', 't', 'i', 'o', 'n', 's', 't', 'h', 'a', 't', 'm', 'a', 'x', 'i', 'm', 'i', 'z', 'e', 't', 'h', 'e', 'i', 'r', 'c', 'h', 'a', 'n', 'c', 'e', 's', 'o', 'f', 'a', 'c', 'h', 'i', 'e', 'v', 'i', 'n', 'g', 'd', 'e', 'f', 'i', 'n', 'e', 'd', 'g', 'o', 'a', 'l', 's', '.', 's', 'u', 'c', 'h', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 's', 'm', 'a', 'y', 'b', 'e', 'c', 'a', 'l', 'l', 'e', 'd', 'a', 'i', 's', '.', 'a', 'i', 't', 'e', 'c', 'h', 'n', 'o', 'l', 'o', 'g', 'y', 'i', 's', 'w', 'i', 'd', 'e', 'l', 'y', 'u', 's', 'e', 'd', 't', 'h', 'r', 'o', 'u', 'g', 'h', 'o', 'u', 't', 'i', 'n', 'd', 'u', 's', 't', 'r', 'y', 'g', 'o', 'v', 'e', 'r', 'n', 'm', 'e', 'n', 't', 'a', 'n', 'd', 's', 'c', 'i', 'e', 'n', 'c', 'e', '.', 's', 'o', 'm', 'e', 'h', 'i', 'g', 'h', '-', 'p', 'r', 'o', 'f', 'i', 'l', 'e', 'a', 'p', 'p', 'l', 'i', 'c', 'a', 't', 'i', 'o', 'n', 's', 'i', 'n', 'c', 'l', 'u', 'd', 'e', 'a', 'd', 'v', 'a', 'n', 'c', 'e', 'd', 'w', 'e', 'b', 's', 'e', 'a', 'r', 'c', 'h', 'e', 'n', 'g', 'i', 'n', 'e', 's', '(', 'e', '.', 'g', '.', 'g', 'o', 'o', 'g', 'l', 'e', 's', 'e', 'a', 'r', 'c', 'h', ')', ';', 'r', 'e', 'c', 'o', 'm', 'm', 'e', 'n', 'd', 'a', 't', 'i', 'o', 'n', 's', 'y', 's', 't', 'e', 'm', 's', '(', 'u', 's', 'e', 'd', 'b', 'y', 'y', 'o', 'u', 't', 'u', 'b', 'e', 'a', 'm', 'a', 'z', 'o', 'n', 'a', 'n', 'd', 'n', 'e', 't', 'f', 'l', 'i', 'x', ')', ';', 'i', 'n', 't', 'e', 'r', 'a', 'c', 't', 'i', 'n', 'g', 'v', 'i', 'a', 'h', 'u', 'm', 'a', 'n', 's', 'p', 'e', 'e', 'c', 'h', '(', 'e', '.', 'g', '.', 'g', 'o', 'o', 'g', 'l', 'e', 'a', 's', 's', 'i', 's', 't', 'a', 'n', 't', 's', 'i', 'r', 'i', 'a', 'n', 'd', 'a', 'l', 'e', 'x', 'a', ')', ';', 'a', 'u', 't', 'o', 'n', 'o', 'm', 'o', 'u', 's', 'v', 'e', 'h', 'i', 'c', 'l', 'e', 's', '(', 'e', '.', 'g', '.', 'w', 'a', 'y', 'm', 'o', ')', ';', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'i', 'v', 'e', 'a', 'n', 'd', 'c', 'r', 'e', 'a', 't', 'i', 'v', 'e', 't', 'o', 'o', 'l', 's', '(', 'e', '.', 'g', '.', 'c', 'h', 'a', 't', 'g', 'p', 't', 'a', 'n', 'd', 'a', 'i', 'a', 'r', 't', ')', ';', 'a', 'n', 'd', 's', 'u', 'p', 'e', 'r', 'h', 'u', 'm', 'a', 'n', 'p', 'l', 'a', 'y', 'a', 'n', 'd', 'a', 'n', 'a', 'l', 'y', 's', 'i', 's', 'i', 'n', 's', 't', 'r', 'a', 't', 'e', 'g', 'y', 'g', 'a', 'm', 'e', 's', '(', 'e', '.', 'g', '.', 'c', 'h', 'e', 's', 's', 'a', 'n', 'd', 'g', 'o', ')', '.', 'h', 'o', 'w', 'e', 'v', 'e', 'r', 'm', 'a', 'n', 'y', 'a', 'i', 'a', 'p', 'p', 'l', 'i', 'c', 'a', 't', 'i', 'o', 'n', 's', 'a', 'r', 'e', 'n', 'o', 't', 'p', 'e', 'r', 'c', 'e', 'i', 'v', 'e', 'd', 'a', 's', 'a', 'i', ':', '\"', 'a', 'l', 'o', 't', 'o', 'f', 'c', 'u', 't', 't', 'i', 'n', 'g', 'e', 'd', 'g', 'e', 'a', 'i', 'h', 'a', 's', 'f', 'i', 'l', 't', 'e', 'r', 'e', 'd', 'i', 'n', 't', 'o', 'g', 'e', 'n', 'e', 'r', 'a', 'l', 'a', 'p', 'p', 'l', 'i', 'c', 'a', 't', 'i', 'o', 'n', 's', 'o', 'f', 't', 'e', 'n', 'w', 'i', 't', 'h', 'o', 'u', 't', 'b', 'e', 'i', 'n', 'g', 'c', 'a', 'l', 'l', 'e', 'd', 'a', 'i', 'b', 'e', 'c', 'a', 'u', 's', 'e', 'o', 'n', 'c', 'e', 's', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', 'b', 'e', 'c', 'o', 'm', 'e', 's', 'u', 's', 'e', 'f', 'u', 'l', 'e', 'n', 'o', 'u', 'g', 'h', 'a', 'n', 'd', 'c', 'o', 'm', 'm', 'o', 'n', 'e', 'n', 'o', 'u', 'g', 'h', 'i', 't', 's', 'n', 'o', 't', 'l', 'a', 'b', 'e', 'l', 'e', 'd', 'a', 'i', 'a', 'n', 'y', 'm', 'o', 'r', 'e', '.', '\"', 'a', 'l', 'a', 'n', 't', 'u', 'r', 'i', 'n', 'g', 'w', 'a', 's', 't', 'h', 'e', 'f', 'i', 'r', 's', 't', 'p', 'e', 'r', 's', 'o', 'n', 't', 'o', 'c', 'o', 'n', 'd', 'u', 'c', 't', 's', 'u', 'b', 's', 't', 'a', 'n', 't', 'i', 'a', 'l', 'r', 'e', 's', 'e', 'a', 'r', 'c', 'h', 'i', 'n', 't', 'h', 'e', 'f', 'i', 'e', 'l', 'd', 't', 'h', 'a', 't', 'h', 'e', 'c', 'a', 'l', 'l', 'e', 'd', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', '.', 'a', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', 'w', 'a', 's', 'f', 'o', 'u', 'n', 'd', 'e', 'd', 'a', 's', 'a', 'n', 'a', 'c', 'a', 'd', 'e', 'm', 'i', 'c', 'd', 'i', 's', 'c', 'i', 'p', 'l', 'i', 'n', 'e', 'i', 'n', '1', '9', '5', '6', '.', 't', 'h', 'e', 'f', 'i', 'e', 'l', 'd', 'w', 'e', 'n', 't', 't', 'h', 'r', 'o', 'u', 'g', 'h', 'm', 'u', 'l', 't', 'i', 'p', 'l', 'e', 'c', 'y', 'c', 'l', 'e', 's', 'o', 'f', 'o', 'p', 't', 'i', 'm', 'i', 's', 'm', 'f', 'o', 'l', 'l', 'o', 'w', 'e', 'd', 'b', 'y', 'p', 'e', 'r', 'i', 'o', 'd', 's', 'o', 'f', 'd', 'i', 's', 'a', 'p', 'p', 'o', 'i', 'n', 't', 'm', 'e', 'n', 't', 'a', 'n', 'd', 'l', 'o', 's', 's', 'o', 'f', 'f', 'u', 'n', 'd', 'i', 'n', 'g', 'k', 'n', 'o', 'w', 'n', 'a', 's', 'a', 'i', 'w', 'i', 'n', 't', 'e', 'r', '.', 'f', 'u', 'n', 'd', 'i', 'n', 'g', 'a', 'n', 'd', 'i', 'n', 't', 'e', 'r', 'e', 's', 't', 'v', 'a', 's', 't', 'l', 'y', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 'd', 'a', 'f', 't', 'e', 'r', '2', '0', '1', '2', 'w', 'h', 'e', 'n', 'd', 'e', 'e', 'p', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', 's', 'u', 'r', 'p', 'a', 's', 's', 'e', 'd', 'a', 'l', 'l', 'p', 'r', 'e', 'v', 'i', 'o', 'u', 's', 'a', 'i', 't', 'e', 'c', 'h', 'n', 'i', 'q', 'u', 'e', 's', 'a', 'n', 'd', 'a', 'f', 't', 'e', 'r', '2', '0', '1', '7', 'w', 'i', 't', 'h', 't', 'h', 'e', 't', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'e', 'r', 'a', 'r', 'c', 'h', 'i', 't', 'e', 'c', 't', 'u', 'r', 'e', '.', 't', 'h', 'i', 's', 'l', 'e', 'd', 't', 'o', 't', 'h', 'e', 'a', 'i', 'b', 'o', 'o', 'm', 'o', 'f', 't', 'h', 'e', 'e', 'a', 'r', 'l', 'y', '2', '0', '2', '0', 's', 'w', 'i', 't', 'h', 'c', 'o', 'm', 'p', 'a', 'n', 'i', 'e', 's', 'u', 'n', 'i', 'v', 'e', 'r', 's', 'i', 't', 'i', 'e', 's', 'a', 'n', 'd', 'l', 'a', 'b', 'o', 'r', 'a', 't', 'o', 'r', 'i', 'e', 's', 'o', 'v', 'e', 'r', 'w', 'h', 'e', 'l', 'm', 'i', 'n', 'g', 'l', 'y', 'b', 'a', 's', 'e', 'd', 'i', 'n', 't', 'h', 'e', 'u', 'n', 'i', 't', 'e', 'd', 's', 't', 'a', 't', 'e', 's', 'p', 'i', 'o', 'n', 'e', 'e', 'r', 'i', 'n', 'g', 's', 'i', 'g', 'n', 'i', 'f', 'i', 'c', 'a', 'n', 't', 'a', 'd', 'v', 'a', 'n', 'c', 'e', 's', 'i', 'n', 'a', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', '.', 'd', 'e', 'e', 'p', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', 'i', 's', 't', 'h', 'e', 's', 'u', 'b', 's', 'e', 't', 'o', 'f', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', 'm', 'e', 't', 'h', 'o', 'd', 's', 'b', 'a', 's', 'e', 'd', 'o', 'n', 'n', 'e', 'u', 'r', 'a', 'l', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', 'w', 'i', 't', 'h', 'r', 'e', 'p', 'r', 'e', 's', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', '.', 't', 'h', 'e', 'a', 'd', 'j', 'e', 'c', 't', 'i', 'v', 'e', '\"', 'd', 'e', 'e', 'p', '\"', 'r', 'e', 'f', 'e', 'r', 's', 't', 'o', 't', 'h', 'e', 'u', 's', 'e', 'o', 'f', 'm', 'u', 'l', 't', 'i', 'p', 'l', 'e', 'l', 'a', 'y', 'e', 'r', 's', 'i', 'n', 't', 'h', 'e', 'n', 'e', 't', 'w', 'o', 'r', 'k', '.', 'm', 'e', 't', 'h', 'o', 'd', 's', 'u', 's', 'e', 'd', 'c', 'a', 'n', 'b', 'e', 'e', 'i', 't', 'h', 'e', 'r', 's', 'u', 'p', 'e', 'r', 'v', 'i', 's', 'e', 'd', 's', 'e', 'm', 'i', '-', 's', 'u', 'p', 'e', 'r', 'v', 'i', 's', 'e', 'd', 'o', 'r', 'u', 'n', 's', 'u', 'p', 'e', 'r', 'v', 'i', 's', 'e', 'd', '.', 'd', 'e', 'e', 'p', '-', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', 'a', 'r', 'c', 'h', 'i', 't', 'e', 'c', 't', 'u', 'r', 'e', 's', 's', 'u', 'c', 'h', 'a', 's', 'd', 'e', 'e', 'p', 'n', 'e', 'u', 'r', 'a', 'l', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', 'd', 'e', 'e', 'p', 'b', 'e', 'l', 'i', 'e', 'f', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', 'r', 'e', 'c', 'u', 'r', 'r', 'e', 'n', 't', 'n', 'e', 'u', 'r', 'a', 'l', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', 'c', 'o', 'n', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'a', 'l', 'n', 'e', 'u', 'r', 'a', 'l', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', 'a', 'n', 'd', 't', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'e', 'r', 's', 'h', 'a', 'v', 'e', 'b', 'e', 'e', 'n', 'a', 'p', 'p', 'l', 'i', 'e', 'd', 't', 'o', 'f', 'i', 'e', 'l', 'd', 's', 'i', 'n', 'c', 'l', 'u', 'd', 'i', 'n', 'g', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', 'v', 'i', 's', 'i', 'o', 'n', 's', 'p', 'e', 'e', 'c', 'h', 'r', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', 'n', 'a', 't', 'u', 'r', 'a', 'l', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 't', 'r', 'a', 'n', 's', 'l', 'a', 't', 'i', 'o', 'n', 'b', 'i', 'o', 'i', 'n', 'f', 'o', 'r', 'm', 'a', 't', 'i', 'c', 's', 'd', 'r', 'u', 'g', 'd', 'e', 's', 'i', 'g', 'n', 'm', 'e', 'd', 'i', 'c', 'a', 'l', 'i', 'm', 'a', 'g', 'e', 'a', 'n', 'a', 'l', 'y', 's', 'i', 's', 'c', 'l', 'i', 'm', 'a', 't', 'e', 's', 'c', 'i', 'e', 'n', 'c', 'e', 'm', 'a', 't', 'e', 'r', 'i', 'a', 'l', 'i', 'n', 's', 'p', 'e', 'c', 't', 'i', 'o', 'n', 'a', 'n', 'd', 'b', 'o', 'a', 'r', 'd', 'g', 'a', 'm', 'e', 'p', 'r', 'o', 'g', 'r', 'a', 'm', 's', 'w', 'h', 'e', 'r', 'e', 't', 'h', 'e', 'y', 'h', 'a', 'v', 'e', 'p', 'r', 'o', 'd', 'u', 'c', 'e', 'd', 'r', 'e', 's', 'u', 'l', 't', 's', 'c', 'o', 'm', 'p', 'a', 'r', 'a', 'b', 'l', 'e', 't', 'o', 'a', 'n', 'd', 'i', 'n', 's', 'o', 'm', 'e', 'c', 'a', 's', 'e', 's', 's', 'u', 'r', 'p', 'a', 's', 's', 'i', 'n', 'g', 'h', 'u', 'm', 'a', 'n', 'e', 'x', 'p', 'e', 'r', 't', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', '.', 'e', 'a', 'r', 'l', 'y', 'f', 'o', 'r', 'm', 's', 'o', 'f', 'n', 'e', 'u', 'r', 'a', 'l', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', 'w', 'e', 'r', 'e', 'i', 'n', 's', 'p', 'i', 'r', 'e', 'd', 'b', 'y', 'i', 'n', 'f', 'o', 'r', 'm', 'a', 't', 'i', 'o', 'n', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', 'a', 'n', 'd', 'd', 'i', 's', 't', 'r', 'i', 'b', 'u', 't', 'e', 'd', 'c', 'o', 'm', 'm', 'u', 'n', 'i', 'c', 'a', 't', 'i', 'o', 'n', 'n', 'o', 'd', 'e', 's', 'i', 'n', 'b', 'i', 'o', 'l', 'o', 'g', 'i', 'c', 'a', 'l', 's', 'y', 's', 't', 'e', 'm', 's', 'i', 'n', 'p', 'a', 'r', 't', 'i', 'c', 'u', 'l', 'a', 'r', 't', 'h', 'e', 'h', 'u', 'm', 'a', 'n', 'b', 'r', 'a', 'i', 'n', '.', 'h', 'o', 'w', 'e', 'v', 'e', 'r', 'c', 'u', 'r', 'r', 'e', 'n', 't', 'n', 'e', 'u', 'r', 'a', 'l', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', 'd', 'o', 'n', 'o', 't', 'i', 'n', 't', 'e', 'n', 'd', 't', 'o', 'm', 'o', 'd', 'e', 'l', 't', 'h', 'e', 'b', 'r', 'a', 'i', 'n', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', 'o', 'f', 'o', 'r', 'g', 'a', 'n', 'i', 's', 'm', 's', 'a', 'n', 'd', 'a', 'r', 'e', 'g', 'e', 'n', 'e', 'r', 'a', 'l', 'l', 'y', 's', 'e', 'e', 'n', 'a', 's', 'l', 'o', 'w', 'q', 'u', 'a', 'l', 'i', 't', 'y', 'm', 'o', 'd', 'e', 'l', 's', 'f', 'o', 'r', 't', 'h', 'a', 't', 'p', 'u', 'r', 'p', 'o', 's', 'e', '.', '=', '=', 'o', 'v', 'e', 'r', 'v', 'i', 'e', 'w', '=', '=', 'm', 'o', 's', 't', 'm', 'o', 'd', 'e', 'r', 'n', 'd', 'e', 'e', 'p', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', 'm', 'o', 'd', 'e', 'l', 's', 'a', 'r', 'e', 'b', 'a', 's', 'e', 'd', 'o', 'n', 'm', 'u', 'l', 't', 'i', '-', 'l', 'a', 'y', 'e', 'r', 'e', 'd', 'n', 'e', 'u', 'r', 'a', 'l', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', 's', 'u', 'c', 'h', 'a', 's', 'c', 'o', 'n', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'a', 'l', 'n', 'e', 'u', 'r', 'a', 'l', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', 'a', 'n', 'd', 't', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'e', 'r', 's', 'a', 'l', 't', 'h', 'o', 'u', 'g', 'h', 't', 'h', 'e', 'y', 'c', 'a', 'n', 'a', 'l', 's', 'o', 'i', 'n', 'c', 'l', 'u', 'd', 'e', 'p', 'r', 'o', 'p', 'o', 's', 'i', 't', 'i', 'o', 'n', 'a', 'l', 'f', 'o', 'r', 'm', 'u', 'l', 'a', 's', 'o', 'r', 'l', 'a', 't', 'e', 'n', 't', 'v', 'a', 'r', 'i', 'a', 'b', 'l', 'e', 's', 'o', 'r', 'g', 'a', 'n', 'i', 'z', 'e', 'd', 'l', 'a', 'y', 'e', 'r', '-', 'w', 'i', 's', 'e', 'i', 'n', 'd', 'e', 'e', 'p', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'i', 'v', 'e', 'm', 'o', 'd', 'e', 'l', 's', 's', 'u', 'c', 'h', 'a', 's', 't', 'h', 'e', 'n', 'o', 'd', 'e', 's', 'i', 'n', 'd', 'e', 'e', 'p', 'b', 'e', 'l', 'i', 'e', 'f', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', 'a', 'n', 'd', 'd', 'e', 'e', 'p', 'b', 'o', 'l', 't', 'z', 'm', 'a', 'n', 'n', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 's', '.', 'f', 'u', 'n', 'd', 'a', 'm', 'e', 'n', 't', 'a', 'l', 'l', 'y', 'd', 'e', 'e', 'p', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', 'r', 'e', 'f', 'e', 'r', 's', 't', 'o', 'a', 'c', 'l', 'a', 's', 's', 'o', 'f', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's', 'i', 'n', 'w', 'h', 'i', 'c', 'h', 'a', 'h', 'i', 'e', 'r', 'a', 'r', 'c', 'h', 'y', 'o', 'f', 'l', 'a', 'y', 'e', 'r', 's', 'i', 's', 'u', 's', 'e', 'd', 't', 'o', 't', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'i', 'n', 'p', 'u', 't', 'd', 'a', 't', 'a', 'i', 'n', 't', 'o', 'a', 's', 'l', 'i', 'g', 'h', 't', 'l', 'y', 'm', 'o', 'r', 'e', 'a', 'b', 's', 't', 'r', 'a', 'c', 't', 'a', 'n', 'd', 'c', 'o', 'm', 'p', 'o', 's', 'i', 't', 'e', 'r', 'e', 'p', 'r', 'e', 's', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n', '.', 'f', 'o', 'r', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 'i', 'n', 'a', 'n', 'i', 'm', 'a', 'g', 'e', 'r', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', 'm', 'o', 'd', 'e', 'l', 't', 'h', 'e', 'r', 'a', 'w', 'i', 'n', 'p', 'u', 't', 'm', 'a', 'y', 'b', 'e', 'a', 'n', 'i', 'm', 'a', 'g', 'e', '(', 'r', 'e', 'p', 'r', 'e', 's', 'e', 'n', 't', 'e', 'd', 'a', 's', 'a', 't', 'e', 'n', 's', 'o', 'r', 'o', 'f', 'p', 'i', 'x', 'e', 'l', 's', ')', '.', 't', 'h', 'e', 'f', 'i', 'r', 's', 't', 'r', 'e', 'p', 'r', 'e', 's', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'l', 'a', 'y', 'e', 'r', 'm', 'a', 'y', 'a', 't', 't', 'e', 'm', 'p', 't', 't', 'o', 'i', 'd', 'e', 'n', 't', 'i', 'f', 'y', 'b', 'a', 's', 'i', 'c', 's', 'h', 'a', 'p', 'e', 's', 's', 'u', 'c', 'h', 'a', 's', 'l', 'i', 'n', 'e', 's', 'a', 'n', 'd', 'c', 'i', 'r', 'c', 'l', 'e', 's', 't', 'h', 'e', 's', 'e', 'c', 'o', 'n', 'd', 'l', 'a', 'y', 'e', 'r', 'm', 'a', 'y', 'c', 'o', 'm', 'p', 'o', 's', 'e', 'a', 'n', 'd', 'e', 'n', 'c', 'o', 'd', 'e', 'a', 'r', 'r', 'a', 'n', 'g', 'e', 'm', 'e', 'n', 't', 's', 'o', 'f', 'e', 'd', 'g', 'e', 's', 't', 'h', 'e', 't', 'h', 'i', 'r', 'd', 'l', 'a', 'y', 'e', 'r', 'm', 'a', 'y', 'e', 'n', 'c', 'o', 'd', 'e', 'a', 'n', 'o', 's', 'e', 'a', 'n', 'd', 'e', 'y', 'e', 's', 'a', 'n', 'd', 't', 'h', 'e', 'f', 'o', 'u', 'r', 't', 'h', 'l', 'a', 'y', 'e', 'r', 'm', 'a', 'y', 'r', 'e', 'c', 'o', 'g', 'n', 'i', 'z', 'e', 't', 'h', 'a', 't', 't', 'h', 'e', 'i', 'm', 'a', 'g', 'e', 'c', 'o', 'n', 't', 'a', 'i', 'n', 's', 'a', 'f', 'a', 'c', 'e', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(list(embeddings_dict.values()))\n"
      ],
      "metadata": {
        "id": "AKL6eyqXi_GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define RNN model for character-level prediction\n",
        "model = Sequential([\n",
        "    SimpleRNN(units=64, input_shape=(1, embedding_dim), return_sequences=True),\n",
        "    Dense(num_unique_chars, activation='softmax')  # Output layer predicts the next character\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Prepare input and output data for training\n",
        "X = np.array([np.expand_dims(embeddings_dict[char], axis=0) for char in data_characters[:-1]])\n",
        "y = np.array([char_to_index[char] for char in data_characters[1:]])  # Labels are indices of next characters\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train2, y_train2, epochs=10, batch_size=32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sck9Wj5jI5k",
        "outputId": "ea72f9db-9c82-4c80-a1e7-a7057d113f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "80/80 [==============================] - 3s 5ms/step - loss: 3.1188\n",
            "Epoch 2/10\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 2.8852\n",
            "Epoch 3/10\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 2.8122\n",
            "Epoch 4/10\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 2.7519\n",
            "Epoch 5/10\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 2.7125\n",
            "Epoch 6/10\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 2.6783\n",
            "Epoch 7/10\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 2.6550\n",
            "Epoch 8/10\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 2.6375\n",
            "Epoch 9/10\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 2.6184\n",
            "Epoch 10/10\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 2.6086\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7faac0068760>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "loss = model.evaluate(X_test2, y_test2)\n",
        "print(\"Test loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oizcVQ7coqCA",
        "outputId": "355d12ef-9a01-4cd7-ba70-1e3a070db577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 0s 3ms/step - loss: 2.7006\n",
            "Test loss: 2.70061993598938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_character(char, model, embeddings_dict):\n",
        "    char_embedding = embeddings_dict[char]\n",
        "    char_embedding = np.expand_dims(char_embedding, axis=0)  # Add batch dimension\n",
        "    char_embedding = np.expand_dims(char_embedding, axis=0)  # Add sequence dimension\n",
        "\n",
        "    next_char_embedding = model.predict(char_embedding)\n",
        "    next_char_index = np.argmax(next_char_embedding)\n",
        "\n",
        "    return index_to_char[next_char_index]\n",
        "\n",
        "# Example usage\n",
        "seed_character = \"i\"\n",
        "predicted_character = predict_next_character(seed_character, model, embeddings_dict)\n",
        "print(\"Predicted next character:\", predicted_character)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgvw5m13ovje",
        "outputId": "040ae0f0-ea1b-4a1c-ccf6-12ecb0245e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 190ms/step\n",
            "Predicted next character: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* The second architecture\n"
      ],
      "metadata": {
        "id": "QNiEg0oLRUzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "if X_train.size == 0 or np.all(X_train == 0):\n",
        "    print(\"Training data is empty. Cannot train the model.\")\n",
        "else:\n",
        "    # Create labels by shifting X_train by one step\n",
        "    y_train = np.roll(X_train, -1, axis=1)\n",
        "    # The last element in y_train will be the same as the first element in X_train, so we remove it\n",
        "    y_train[:, -1, :] = 0\n",
        "\n",
        "    # Assuming X_train contains a single sequence of word embeddings\n",
        "    number_of_words = X_train.shape[1]  # Number of words in the sequence\n",
        "    embedding_dim = X_train.shape[2]  # Embedding dimension\n",
        "\n",
        "    # Define RNN model with an additional layer\n",
        "    model = Sequential([\n",
        "        SimpleRNN(units=64, return_sequences=True, input_shape=(None, embedding_dim)),\n",
        "        SimpleRNN(units=64, return_sequences=True),  # Additional SimpleRNN layer\n",
        "        Dense(embedding_dim)  # Output layer with the same dimension as input (to predict the next word embedding)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mae')  # Use MAE loss for predicting embeddings\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=4)\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "Ig9wVg86Eo0X",
        "outputId": "fbcfdf03-dfff-44c4-b70a-5981bf756ac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.5913\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.5688\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.5542\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.5430\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.5327\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.5224\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.5117\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 397ms/step - loss: 0.5010\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 0.4903\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 0.4800\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_2 (SimpleRNN)    (None, None, 64)          10560     \n",
            "                                                                 \n",
            " simple_rnn_3 (SimpleRNN)    (None, None, 64)          8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, None, 100)         6500      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25316 (98.89 KB)\n",
            "Trainable params: 25316 (98.89 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model summary to PDF for the second architecture\n",
        "save_model_summary_to_pdf(model, 'simpleRNN')\n"
      ],
      "metadata": {
        "id": "p_DukfA9Z07v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape X_test and X_test_labels to match the expected input shape\n",
        "\n",
        "# Prepare labels for the test set\n",
        "X_test_labels = np.roll(X_test, -1, axis=1)\n",
        "X_test_labels[:, -1, :] = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "4MsQjXpyFGli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = model.evaluate(X_test, X_test_labels)  # Assuming reconstruction loss\n",
        "print(\"Test loss:\", loss)"
      ],
      "metadata": {
        "id": "Ww9kGVCVFLjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a0dc6b-0d84-402f-9003-f80da5add40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 460ms/step - loss: 0.4911\n",
            "Test loss: 0.49112769961357117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prediction  Based Character"
      ],
      "metadata": {
        "id": "GaSCp4lWR6yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Define RNN model for character-level prediction with an additional layer\n",
        "model = Sequential([\n",
        "    SimpleRNN(units=64, input_shape=(1, embedding_dim), return_sequences=True),\n",
        "    SimpleRNN(units=64, return_sequences=True),  # Additional SimpleRNN layer\n",
        "    Dropout(0.2),  # Dropout layer to prevent overfitting\n",
        "    Dense(num_unique_chars, activation='softmax')  # Output layer predicts the next character\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Prepare input and output data for training\n",
        "X = np.array([np.expand_dims(embeddings_dict[char], axis=0) for char in data_characters[:-1]])\n",
        "y = np.array([char_to_index[char] for char in data_characters[1:]])  # Labels are indices of next characters\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train2, y_train2, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "RXo85hnyE68u",
        "outputId": "e436f8a4-eee2-4718-953b-1ed537d81f8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "80/80 [==============================] - 4s 7ms/step - loss: 3.1362\n",
            "Epoch 2/10\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 2.9084\n",
            "Epoch 3/10\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 2.8150\n",
            "Epoch 4/10\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 2.7465\n",
            "Epoch 5/10\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 2.7117\n",
            "Epoch 6/10\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 2.6875\n",
            "Epoch 7/10\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 2.6728\n",
            "Epoch 8/10\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 2.6564\n",
            "Epoch 9/10\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 2.6589\n",
            "Epoch 10/10\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 2.6226\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7faa567935e0>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "loss = model.evaluate(X_test2, y_test2)\n",
        "print(\"Test loss:\", loss)"
      ],
      "metadata": {
        "id": "Z8Zl11ijFkrx",
        "outputId": "eb255ed0-e549-443e-91ca-ff8bcb54a06e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 1s 5ms/step - loss: 2.6899\n",
            "Test loss: 2.6899356842041016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for character-based text generation\n",
        "seed_character = \"a\"  # Starting character\n",
        "iterations = 10\n",
        "\n",
        "# Initialize the seed sequence with the starting characters\n",
        "seed_chars = list(\"artificial intelligenc\")  # Adjust as needed for your seed sequence\n",
        "\n",
        "# Generate text for 10 iterations\n",
        "for _ in range(iterations):\n",
        "    predicted_character = predict_next_character(seed_chars[-1], model, embeddings_dict)\n",
        "    if predicted_character:\n",
        "        seed_chars.append(predicted_character)\n",
        "        print(predicted_character, end='')\n",
        "print()\n"
      ],
      "metadata": {
        "id": "jNygaOCxE26H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73b9be2-5fb7-4083-f812-3607cb1fa390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "e\n"
          ]
        }
      ]
    }
  ]
}