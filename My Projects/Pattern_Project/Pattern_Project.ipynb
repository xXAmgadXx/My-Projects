{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5212576,"sourceType":"datasetVersion","datasetId":3032092}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n\nlabel_encoder = LabelEncoder()\n# Load the CSV file into a DataFrame\ndf = pd.read_csv('/kaggle/input/cancer-data/Cancer_Data.csv')\n\n# Display the first few rows of the DataFrame\nprint(df.head())\n\n# Separate the target and features\n\nX = df.drop(columns=['id', 'diagnosis'])  # Features \ny = label_encoder.fit_transform(df['diagnosis'])  # Target column\n\n# Convert to NumPy arrays\nX = X.to_numpy()\nX = np.delete(X, 30, axis=1) # nan column\n\n# Display the shapes of X and y\nprint(\"Features shape:\", X.shape)\nprint(\"Target shape:\", y.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-21T12:45:08.791403Z","iopub.execute_input":"2024-05-21T12:45:08.792253Z","iopub.status.idle":"2024-05-21T12:45:09.661653Z","shell.execute_reply.started":"2024-05-21T12:45:08.792217Z","shell.execute_reply":"2024-05-21T12:45:09.660721Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0    842302         M        17.99         10.38          122.80     1001.0   \n1    842517         M        20.57         17.77          132.90     1326.0   \n2  84300903         M        19.69         21.25          130.00     1203.0   \n3  84348301         M        11.42         20.38           77.58      386.1   \n4  84358402         M        20.29         14.34          135.10     1297.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n3          0.14250           0.28390          0.2414              0.10520   \n4          0.10030           0.13280          0.1980              0.10430   \n\n   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n0  ...          17.33           184.60      2019.0            0.1622   \n1  ...          23.41           158.80      1956.0            0.1238   \n2  ...          25.53           152.50      1709.0            0.1444   \n3  ...          26.50            98.87       567.7            0.2098   \n4  ...          16.67           152.20      1575.0            0.1374   \n\n   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n0             0.6656           0.7119                0.2654          0.4601   \n1             0.1866           0.2416                0.1860          0.2750   \n2             0.4245           0.4504                0.2430          0.3613   \n3             0.8663           0.6869                0.2575          0.6638   \n4             0.2050           0.4000                0.1625          0.2364   \n\n   fractal_dimension_worst  Unnamed: 32  \n0                  0.11890          NaN  \n1                  0.08902          NaN  \n2                  0.08758          NaN  \n3                  0.17300          NaN  \n4                  0.07678          NaN  \n\n[5 rows x 33 columns]\nFeatures shape: (569, 30)\nTarget shape: (569,)\n","output_type":"stream"}]},{"cell_type":"code","source":"def pairwise_mean_feature_extraction(data):\n    \"\"\"\n    Perform pairwise mean feature extraction on the input data.\n    \n    Parameters:\n    data (numpy.ndarray): Input data where each row represents an observation and each column represents a feature.\n    \n    Returns:\n    numpy.ndarray: Extracted features where each row represents an observation and each column represents the mean of a pair of columns.\n    \"\"\"\n    num_features = data.shape[1]\n    num_pairs = num_features - 1\n    extracted_features = np.zeros((data.shape[0], num_pairs))\n    \n    for i in range(num_pairs):\n        extracted_features[:, i] = np.mean(data[:, i:i+2], axis=1)\n    \n    return extracted_features","metadata":{"execution":{"iopub.status.busy":"2024-05-21T12:45:19.005774Z","iopub.execute_input":"2024-05-21T12:45:19.006124Z","iopub.status.idle":"2024-05-21T12:45:19.012523Z","shell.execute_reply.started":"2024-05-21T12:45:19.006097Z","shell.execute_reply":"2024-05-21T12:45:19.011417Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"X_extracted = pairwise_mean_feature_extraction(X)\nprint(X_extracted.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T12:45:24.139841Z","iopub.execute_input":"2024-05-21T12:45:24.140547Z","iopub.status.idle":"2024-05-21T12:45:24.146775Z","shell.execute_reply.started":"2024-05-21T12:45:24.140514Z","shell.execute_reply":"2024-05-21T12:45:24.145890Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(569, 29)\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install pyswarm","metadata":{"execution":{"iopub.status.busy":"2024-05-21T12:45:29.023009Z","iopub.execute_input":"2024-05-21T12:45:29.023846Z","iopub.status.idle":"2024-05-21T12:45:45.180585Z","shell.execute_reply.started":"2024-05-21T12:45:29.023812Z","shell.execute_reply":"2024-05-21T12:45:45.179249Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting pyswarm\n  Downloading pyswarm-0.6.tar.gz (4.3 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pyswarm) (1.26.4)\nBuilding wheels for collected packages: pyswarm\n  Building wheel for pyswarm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyswarm: filename=pyswarm-0.6-py3-none-any.whl size=4464 sha256=e25b5fbbe798891eab3e94a26588715ebaa5b32f055750f3c692db88d9133987\n  Stored in directory: /root/.cache/pip/wheels/71/67/40/62fa158f497f942277cbab8199b05cb61c571ab324e67ad0d6\nSuccessfully built pyswarm\nInstalling collected packages: pyswarm\nSuccessfully installed pyswarm-0.6\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pyswarm import pso\n\n\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_extracted, y, test_size=0.2, random_state=42)\n\n# Define fitness function\ndef fitness_function(particle, X_train, X_test, y_train, y_test):\n    # Convert particle to a binary mask (threshold at 0.5)\n    mask = particle > 0.5\n    # Ensure at least one feature is selected\n    if np.sum(mask) == 0:\n        return 1.0  # return worst score\n    \n    selected_features_train = X_train[:, mask]\n    selected_features_test = X_test[:, mask]\n    \n    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    clf.fit(selected_features_train, y_train)\n    accuracy = clf.score(selected_features_test, y_test)\n    return -accuracy\n\n\n# Define lower and upper bounds for each feature\nlb = np.zeros(X_train.shape[1], dtype=int)\nub = np.ones(X_train.shape[1], dtype=int)\n\n\n# Perform PSO for feature selection\nbest_particle, _ = pso(fitness_function, lb, ub, args=(X_train, X_test, y_train, y_test), swarmsize=100, maxiter=100)\n\n# Convert the best_particle to a binary mask (threshold at 0.5)\nbest_features_mask = best_particle > 0.5\n\n# Print the indices of the selected features\nselected_feature_indices = np.where(best_features_mask)[0]\nprint(\"Selected features:\", selected_feature_indices)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:33:35.815077Z","iopub.execute_input":"2024-05-21T13:33:35.815478Z","iopub.status.idle":"2024-05-21T14:15:55.293381Z","shell.execute_reply.started":"2024-05-21T13:33:35.815447Z","shell.execute_reply":"2024-05-21T14:15:55.292459Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Stopping search: maximum iterations reached --> 100\nSelected features: [ 0  1  2  3  5  7  9 10 13 14 16 17 21 25 26]\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nX_extracted_selected = X_extracted[: , selected_feature_indices]\nprint(X_extracted_selected.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:31:22.787891Z","iopub.execute_input":"2024-05-21T14:31:22.788272Z","iopub.status.idle":"2024-05-21T14:31:22.794168Z","shell.execute_reply.started":"2024-05-21T14:31:22.788240Z","shell.execute_reply":"2024-05-21T14:31:22.793236Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"(569, 15)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.decomposition import NMF\n\n# Apply Non-negative Matrix Factorization (NMF)\nnmf = NMF(n_components=5)\nX_extracted_reduced = nmf.fit_transform(X_extracted_selected)\n\nprint(X_extracted_reduced.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:32:02.460175Z","iopub.execute_input":"2024-05-21T14:32:02.461153Z","iopub.status.idle":"2024-05-21T14:32:02.474522Z","shell.execute_reply.started":"2024-05-21T14:32:02.461120Z","shell.execute_reply":"2024-05-21T14:32:02.473700Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"(569, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_extracted_reduced, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:32:05.038378Z","iopub.execute_input":"2024-05-21T14:32:05.038725Z","iopub.status.idle":"2024-05-21T14:32:05.045304Z","shell.execute_reply.started":"2024-05-21T14:32:05.038700Z","shell.execute_reply":"2024-05-21T14:32:05.044418Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"class LogisticRegression:\n    def __init__(self, learning_rate=0.001, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        for _ in range(self.n_iterations):\n            linear_model = np.dot(X, self.weights) + self.bias\n            y_predicted = self.sigmoid(linear_model)\n\n            # Gradient descent\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / n_samples) * np.sum(y_predicted - y)\n\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n\n    def predict(self, X):\n        linear_model = np.dot(X, self.weights) + self.bias\n        y_predicted = self.sigmoid(linear_model)\n        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n        return y_predicted_cls","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:32:06.772519Z","iopub.execute_input":"2024-05-21T14:32:06.773576Z","iopub.status.idle":"2024-05-21T14:32:06.782758Z","shell.execute_reply.started":"2024-05-21T14:32:06.773541Z","shell.execute_reply":"2024-05-21T14:32:06.781822Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n\n\n    # Initialize and fit model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Predict\n\n    predictions = model.predict(X_test)\n    #print(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:32:09.247615Z","iopub.execute_input":"2024-05-21T14:32:09.248287Z","iopub.status.idle":"2024-05-21T14:32:09.303081Z","shell.execute_reply.started":"2024-05-21T14:32:09.248253Z","shell.execute_reply":"2024-05-21T14:32:09.302136Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\naccuracy_test = accuracy_score(y_test, predictions)\n\n\nprint(\"Accuracy of the Logistic Classifier on Test Set:\", accuracy_test * 100)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:32:12.367232Z","iopub.execute_input":"2024-05-21T14:32:12.368078Z","iopub.status.idle":"2024-05-21T14:32:12.373792Z","shell.execute_reply.started":"2024-05-21T14:32:12.368042Z","shell.execute_reply":"2024-05-21T14:32:12.372913Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"Accuracy of the Logistic Classifier on Test Set: 71.05263157894737\n","output_type":"stream"}]}]}